{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first approach is from using huggingface endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "if not HUGGINGFACEHUB_API_TOKEN:\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_length=128,\n",
    "    temperature=0.5,\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nirajanpaudel17/Documents/Projects/Langchain/langchain_rag_env/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n## Meaning | Synonyms\\n\\n- a question asking for identification or description of someone or something\\n- a question to determine the nature, kind, or essential character of someone or something\\n- a question to find out what category or label someone or something belongs to\\n- a question to ascertain the identity or origin of someone or something\\n- a question to learn about the essential features or qualities of someone or something\\n- a question to understand the true nature or meaning of something\\n\\n## Example Sentences\\n\\n1. I don\\'t know what you are, but you\\'re not human.\\n2. The detective asked what the suspect was carrying in his bag.\\n3. The biologist wanted to know what species the new discovery belonged to.\\n4. The lawyer asked what kind of damage the plaintiff was claiming.\\n5. The teacher asked the students what they wanted to be when they grew up.\\n6. The scientist was trying to determine what caused the strange phenomenon.\\n7. The art critic wanted to know what genre the painting belonged to.\\n8. The customer service representative asked what type of problem the caller was experiencing.\\n9. The historian was trying to figure out what period in history the artifact came from.\\n10. The doctor asked what symptoms the patient was experiencing.\\n\\n## Origin\\n\\nThe word \"what are you?\" comes from the Old English word \"hw√¶t,\" which means \"what\" or \"what kind of.\" It has been used in English since at least the 14th century. The phrase is often used as a challenge or a question to elicit a response, especially when the speaker is unsure of someone\\'s or something\\'s identity or nature. In literature, it is often used as a rhetorical question or a way to express curiosity or skepticism. For example, in Shakespeare\\'s play \"Othello,\" Iago asks Othello, \"What art thou in this quarrel?\" to provoke a reaction.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm.invoke(\"what are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "question = \"What is the purpose of human life?\"\n",
    "template = \"\"\"\n",
    "You are an AI assistant and should reply to the {question} given\n",
    "question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] input_types={} partial_variables={} template='\\nYou are an AI assistant and should reply to the {question} given\\nquestion: {question}\\n'\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='\\nYou are an AI assistant and should reply to the {question} given\\nquestion: {question}\\n') middle=[] last=HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', huggingfacehub_api_token='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', temperature=0.5, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 128}, model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>)\n"
     ]
    }
   ],
   "source": [
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nirajanpaudel17/Documents/Projects/Langchain/langchain_rag_env/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# response = chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'm an AI language model and don't have the ability to hold beliefs or have a purpose. However, I can provide information on various philosophical and spiritual perspectives on the purpose of human life.\n",
      "\n",
      "Throughout history, different cultures and religions have offered their own interpretations on the meaning and purpose of human life. Some believe that the purpose is to seek knowledge and wisdom, others believe it is to serve and help others, and still others believe it is to find happiness and fulfillment.\n",
      "\n",
      "From a scientific or atheistic perspective, some people believe that human life has no inherent purpose beyond what we make of it. They argue that we are just complex biological organisms that have evolved through natural selection, and that our purpose is to survive and reproduce.\n",
      "\n",
      "Ultimately, the question of the purpose of human life is a deeply personal and subjective one, and there is no definitive answer. It is a question that each individual must grapple with for themselves, based on their own beliefs, values, and experiences.\n"
     ]
    }
   ],
   "source": [
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 approach is using huggingface pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
